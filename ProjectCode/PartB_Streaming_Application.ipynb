{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Sohan Pujar\n",
    "Student Code: 30567556\n",
    "Email: spuj0001@student.monash.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import appropriate dependencies and libraries\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.0 pyspark-shell'\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import pandas\n",
    "from pymongo import MongoClient\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark.sql import SparkSession\n",
    "from pprint import pprint\n",
    "\n",
    "# Looked at source code for pygeohash as couldn't import pygeohash appropriately\n",
    "def encode(latitude, longitude, precision=12):\n",
    "    \"\"\"\n",
    "    Encode a position given in float arguments latitude, longitude to\n",
    "    a geohash which will have the character count precision.\n",
    "    \"\"\"\n",
    "    __base32 = '0123456789bcdefghjkmnpqrstuvwxyz' #32 characters for base 32 expression\n",
    "    lat_interval = (-90.0, 90.0)\n",
    "    lon_interval = (-180.0, 180.0)\n",
    "    geohash = []\n",
    "    bits = [16, 8, 4, 2, 1]\n",
    "    bit = 0\n",
    "    ch = 0\n",
    "    even = True\n",
    "    while len(geohash) < precision:\n",
    "        if even:\n",
    "            mid = (lon_interval[0] + lon_interval[1]) / 2\n",
    "            if longitude > mid:\n",
    "                ch |= bits[bit]\n",
    "                lon_interval = (mid, lon_interval[1])\n",
    "            else:\n",
    "                lon_interval = (lon_interval[0], mid)\n",
    "        else:\n",
    "            mid = (lat_interval[0] + lat_interval[1]) / 2\n",
    "            if latitude > mid:\n",
    "                ch |= bits[bit]\n",
    "                lat_interval = (mid, lat_interval[1])\n",
    "            else:\n",
    "                lat_interval = (lat_interval[0], mid)\n",
    "        even = not even\n",
    "        if bit < 4:\n",
    "            bit += 1\n",
    "        else:\n",
    "            geohash += __base32[ch]\n",
    "            bit = 0\n",
    "            ch = 0\n",
    "    return ''.join(geohash)\n",
    "\n",
    "spark = SparkSession.builder.config(\n",
    "    \"spark.archives\",  \n",
    "    \"pyspark_venv.tar.gz#environment\").getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "def sendDataToDB(iter):\n",
    "    client = MongoClient()\n",
    "    db = client.fit3182_assignment_db\n",
    "    hardquestion = db.hardquestion# Initialising appropriate pymongo commands\n",
    "    hashed = 0\n",
    "    lst = []\n",
    "    proper_lst = []\n",
    "    for record in iter:\n",
    "        data = json.loads(record[1]) # goes through iter\n",
    "        if data.get(\"producer\") == 1: # process data if record comes from producer one\n",
    "            jsonData = {} # form document and process the RDD\n",
    "            jsonData['latitude'] = data.get('latitude')\n",
    "            jsonData['longitude'] = data.get('longitude')\n",
    "            jsonData['air_temperature_celcius'] = data.get('air_temperature_celcius')\n",
    "            jsonData['relative_humidity'] = data.get('relative_humidity')\n",
    "            jsonData['windspeed_knots'] = data.get('windspeed_knots')\n",
    "            jsonData['max_wind_speed'] = data.get('max_wind_speed')\n",
    "            jsonData['GHI_w/m2'] = data.get('GHI_w/m2')\n",
    "            jsonData['report'] = data.get('report')\n",
    "            jsonData['precipitation '] = data.get('precipitation ')\n",
    "            jsonData[\"producer\"] = data.get(\"producer\")\n",
    "            jsonData['date'] = data.get('date') \n",
    "            date = data.get('date') # save as variables for later use\n",
    "            air_temp = data.get('air_temperature_celcius')\n",
    "            GHI = data.get('GHI_w/m2')\n",
    "            hashed = encode(data.get('latitude'),data.get('longitude'),precision = 3) # Call pygeohash source code, encode to define hashed\n",
    "            hardquestion.insert_one(jsonData) # inserting the document in database\n",
    "        else:\n",
    "            lst.append(data) #appends separate list as RDD is lost from iter once iterated \n",
    "    \n",
    "    for data in lst: # iterate over list with RDD from producer 2 and 3\n",
    "        if encode(data.get('latitude'),data.get('longitude'),precision = 3) == hashed:\n",
    "            proper_lst.append(data) # filter out the hotspots which don't lie within region of hashed from climate with precision 3\n",
    "    dictionary = {}\n",
    "    for data in proper_lst: # Iterate over filtered data\n",
    "        hashed = encode(data.get('latitude'),data.get('longitude'),precision = 5)\n",
    "        hash_date = data.get('date')[12:]\n",
    "        key = str(hashed)+hash_date # Create dictionary and store as key\n",
    "        if key in dictionary:\n",
    "            dictionary[key].append(data) # Lies within precision 5 then append\n",
    "        else:\n",
    "            dictionary[key] = [data] # If data doesn't already exist, then create new key\n",
    "    \n",
    "    \n",
    "    for key in dictionary: # Iterate through dictionary\n",
    "        data = dictionary[key][0]\n",
    "        long = data.get('latitude') # grab first value as lat, long and date will be same values per key\n",
    "        lat = data.get('longitude')\n",
    "        hotspot_date = data.get('date')\n",
    "        stc_lst = [] # Initialised and processed for later use\n",
    "        c_lst = []\n",
    "        for record in dictionary[key]:\n",
    "            stc = record.get('surface_temperature_celcius')\n",
    "            c = record.get('confidence')\n",
    "            stc_lst.append(stc)\n",
    "            c_lst.append(c) # Append all records of surface temperature and cofidence to corresponding list\n",
    "        stc_avg = sum(stc_lst)/len(stc_lst)\n",
    "        c_avg = sum(c_lst)/len(c_lst) # Calculate average\n",
    "        if GHI>180 and air_temp > 20: # Check if fire is natural\n",
    "            json_doc = {\"longitude\": long, \"latitude\": lat, \"datetime\": hotspot_date, \"surface_temp_avg\":stc_avg, \"confidence_avg\": c_avg, \"cause_of_fire\": \"natural\"}\n",
    "        else:\n",
    "            json_doc = {\"longitude\": long, \"latitude\": lat, \"datetime\": hotspot_date, \"surface_temp_avg\":stc_avg, \"confidence_avg\": c_avg, \"cause_of_fire\": \"other\"}\n",
    "        hardquestion.update_one({'date':date},{'$push':{'hotspot_location':json_doc}}) # Insert into collection\n",
    "    \n",
    "    client.close() # close client\n",
    "\n",
    "n_secs = 10 # number of seconds before consuming\n",
    "topic = \"data\"\n",
    "\n",
    "conf = SparkConf().setAppName(\"KafkaStreamProcessor\").setMaster(\"local[2]\")\n",
    "sc = SparkContext.getOrCreate() # creating sparkcontext\n",
    "if sc is None:\n",
    "    sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "ssc = StreamingContext(sc, n_secs) # batch window in the spark context\n",
    "    \n",
    "kafkaStream = KafkaUtils.createDirectStream(ssc, [topic], {\n",
    "                        'bootstrap.servers':'127.0.0.1:9092', \n",
    "                        'group.id':'week12-group', \n",
    "                        'fetch.message.max.bytes':'15728640',\n",
    "                        'auto.offset.reset':'largest'})\n",
    "                        # Group ID is completely arbitrary\n",
    "\n",
    "lines = kafkaStream.foreachRDD(lambda rdd: rdd.foreachPartition(sendDataToDB)) # Partitioned RDD to process simultaneously\n",
    "kafkaStream.pprint()# printing the records consumed\n",
    "ssc.start()\n",
    "time.sleep(600) # Run stream for 10 minutes just in case no detection of producer\n",
    "# ssc.awaitTermination()\n",
    "ssc.stop(stopSparkContext=True,stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient()\n",
    "db = client.fit3182_assignment_db\n",
    "db.hardquestion.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode(-30, -30,precision = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
